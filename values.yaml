# Default values for caas kube-prometheus-stack.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
#
#

caas:
  rbac:
    enabled: true
    serviceAccount:
      create: true
      name: project-monitoring
  # list of namespaces where project-monitoring should watch
  #projectNamespaces:
  #projectNamespaces: demoapp3 demoapp4

global:
  cattle:
    systemDefaultRegistry: ""

kube-prometheus-stack:
#  full set of values and pre-defined for caas
#  ref: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
  nameOverride: "project-monitoring"
  fullnameOverride: "project-monitoring"
  #commonLabels:
  #  field.cattle.io/projectId: p-q8bp8

## Create default rules for monitoring the cluster
##
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: false
      configReloaders: true
      general: true
      k8s: true
      kubeApiserverAvailability: false
      kubeApiserverBurnrate: false
      kubeApiserverHistogram: false
      kubeApiserverSlos: false
      kubeControllerManager: false
      kubelet: false
      kubeProxy: false
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: false
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeScheduler: false
      kubeStateMetrics: true
      network: true
      node: false
      nodeExporterAlerting: false
      nodeExporterRecording: false
      prometheus: true
      prometheusOperator: false
  
    appNamespacesTarget: ".*"
    runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"
    ## Disabled PrometheusRule alerts
    disabled: {}
    # KubeAPIDown: true
    # NodeRAIDDegraded: true
  
  ## Provide custom recording or alerting rules to be deployed into the cluster.
  ##
  alertmanager:
    apiVersion: v2
    enabled: true
    config:
      global:
        resolve_timeout: 5m
      inhibit_rules:
        - source_matchers:
            - 'severity = critical'
          target_matchers:
            - 'severity =~ warning|info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'severity = warning'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'alertname = InfoInhibitor'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
      route:
        group_by: ['namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'null'
        routes:
        - receiver: 'null'
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog"
      receivers:
      - name: 'null'
      templates:
      - '/etc/alertmanager/config/*.tmpl'
    serviceAccount:
      create: true
    service:
      annotations: {}
      labels: {}
      clusterIP: ""
      port: 9093
      targetPort: 9093
      externalIPs: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      externalTrafficPolicy: Cluster
      type: ClusterIP
    servicePerReplica:
      enabled: false
    serviceMonitor:
      interval: "30s"
      selfMonitor: true
    alertmanagerSpec:
      alertmanagerConfigNamespaceSelector: {}
      # alertmanagerConfigNamespaceSelector:
      #   matchLabels:
      #     alertmanagerconfig: enabled
      clusterAdvertiseAddress: false
      externalUrl:
      forceEnableClusterMode: false
      image:
        repository: mtr.devops.telekom.de/kubeprometheusstack/alertmanager
        tag: v0.24.0
      listenLocal: false
      logFormat: logfmt
      logLevel: debug
      minReadySeconds: 5
      replicas: 1
      retention: 120h
      storage: {}
      # volumeClaimTemplate:
      #   spec:
      #     storageClassName: gluster
      #     accessModes: ["ReadWriteOnce"]
      #     resources:
      #       requests:
      #         storage: 50Gi
      #     selector: {}
      routePrefix: /
      resources:
        limits:
          memory: 750Mi
          cpu: 800m
        requests:
          memory: 200Mi
          cpu: 100m
      paused: false
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      volumes: []
      volumeMounts: []
  global:
    rbac:
      create: false
      createAggregateClusterRoles: false
      pspEnabled: false
  grafana:
    adminPassword: prom-operator
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      runAsUser: 472
      runAsGroup: 472
      privileged: false
      readOnlyRootFilesystem: true
    createConfigmap: true
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-operated:9090
          access: proxy
          isDefault: true
    defaultDashboardsEnabled: true
    defaultDashboardsTimezone: utc
    enabled: true
    extraContainers: |
      - name: grafana-proxy
        args:
        - nginx
        - -g
        - daemon off;
        - -c
        - /nginx/nginx.conf
        image: mtr.devops.telekom.de/kubeprometheusstack/nginx:1.23.2-alpine
        ports:
        - containerPort: 8080
          name: nginx-http
          protocol: TCP
        volumeMounts:
        - mountPath: /nginx
          name: grafana-nginx
        - mountPath: /var/cache/nginx
          name: nginx-home
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          runAsUser: 101
          runAsGroup: 101
          readOnlyRootFilesystem: true
    extraContainerVolumes:
      - name: nginx-home
        emptyDir: {}
      - name: grafana-nginx
        configMap:
          name: nginx-proxy-config-project-monitoring-grafana
          items:
          - key: nginx.conf
            mode: 438
            path: nginx.conf
    forceDeployDatasources: true
    forceDeployDashboards: true
    fullnameOverride: project-monitoring-grafana
    grafana.ini:
      analytics:
        check_for_updates = false
      users:
        auto_assign_org_role: Viewer
      auth:
        disable_login_form: false
      auth.anonymous:
        enabled: true
        org_role: Viewer
      auth.basic:
        enabled: false
      security:
        # Required to embed dashboards in Rancher Cluster Overview Dashboard on Cluster Explorer
        allow_embedding: true
      log:
        level: debug
    image:
      repository: mtr.devops.telekom.de/kubeprometheusstack/grafana
      tag: 9.1.5
    namespaceOverride: ""
    nameOverride: project-monitoring-grafana
    securityContext:
      fsGroup: 472
      runAsUser: 472
      runAsGroup: 472
      supplementalGroups:
      - 472
    rbac:
      create: false
      namespaced: true
      pspEnabled: false
    serviceAccount:
      create: false
      name: project-monitoring
    resources:
      limits:
        memory: 600Mi
        cpu: 600m
      requests:
        memory: 200Mi
        cpu: 200m
    sidecar:
      image:
        repository: mtr.devops.telekom.de/kubeprometheusstack/k8s-sidecar
        tag: 1.19.2
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsUser: 472
        runAsGroup: 472
        privileged: false
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        annotations: {}
        multicluster:
          global:
            enabled: false
          etcd:
            enabled: false
        provider:
          allowUiUpdates: false
      datasources:
        createPrometheusReplicasDatasources: false
        defaultDatasourceEnabled: true
        enabled: true
        label: grafana_datasource
        labelValue: "1"
        searchNamespace: ""
    serviceMonitor:
      enabled: true
      path: "/metrics"
      interval: "30s"
      scheme: http
      scrapeTimeout: 30s
    service:
      portName: nginx-http
      port: 80
      targetPort: 8080
    testFramework:
      enabled: false
  kubeApiServer:
    enabled: false
  kubelet:
    enabled: false
  kubeControllerManager:
    enabled: false
  coreDns:
    enabled: false
  kubeDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeProxy:
    enabled: false
  kubeStateMetrics:
    enabled: false
  kube-state-metrics:
    rbac:
      create: false
    releaseLabel: false
    prometheus:
      monitor:
        enabled: false
        honorLabels: false
    selfMonitor:
      enabled: false
  nodeExporter:
    enabled: false
  prometheus-node-exporter:
    prometheus:
      monitor:
        enabled: false
    releaseLabel: false
    rbac:
      pspEnabled: false
  prometheusOperator:
    enabled: false
  prometheus:
    enabled: true
    ingress:
      enabled: false
      annotations: {}
      labels: {}
      hosts: []
      paths: []
      tls: []
    ingressPerReplica:
      enabled: false
    prometheusSpec:
      additionalRemoteRead: []
      additionalRemoteWrite: []
      additionalScrapeConfigs:
        - job_name: 'federate'
          scrape_interval: 10s
          honor_labels: true
          metrics_path: '/federate'
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          static_configs:
            - targets:
              - rancher-monitoring-prometheus.cattle-monitoring-system.svc:9091
          params:
            'match[]':
              - '{__name__=~".+"}'
      apiserverConfig: {}
      disableCompaction: false
      enableAdminAPI: false
      evaluationInterval: ""
      externalLabels: {}
      externalUrl: ""
      excludedFromEnforcement: []
      enforcedNamespaceLabel: ""
      enforcedLabelLimit: false
      enforcedLabelNameLengthLimit: false
      enforcedLabelValueLengthLimit: false
      enforcedSampleLimit: false
      enforcedTargetLimit: false
      enableRemoteWriteReceiver: false
      image:
        repository: mtr.devops.telekom.de/kubeprometheusstack/prometheus
        tag: v2.38.0
      ignoreNamespaceSelectors: false
      listenLocal: false
      logFormat: logfmt
      logLevel: debug
      overrideHonorLabels: false
      overrideHonorTimestamps: false
      portName: "http-web"
      prometheusRulesExcludedFromEnforce: []
      prometheusExternalLabelName: ""
      prometheusExternalLabelNameClear: false
      podAntiAffinityTopologyKey: kubernetes.io/hostname
      podMonitorSelectorNilUsesHelmValues: false
      #podMonitorSelector: {}
      podMonitorNamespaceSelector:
        matchLabels:
          field.cattle.io/projectId: p-v7sr7
      probeSelectorNilUsesHelmValues: false
      probeNamespaceSelector: {}
      paused: false
      queryLogFile: false
      query: {}
      replicas: 1
      remoteRead: []
      remoteWrite: []
      remoteWriteDashboards: false
      resources:
        limits:
          memory: 1000Mi
          cpu: 1000m
        requests:
          memory: 400Mi
          cpu: 400m
      retention: 10d
      retentionSize: ""
      routePrefix: /
      replicaExternalLabelName: ""
      replicaExternalLabelNameClear: false
      ruleSelectorNilUsesHelmValues: true
      ruleSelector: {}
      scrapeInterval: "60s"
      scrapeTimeout: "10s"
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      serviceMonitorSelectorNilUsesHelmValues: true
      # serviceMonitorSelector: {}
      serviceMonitorNamespaceSelector:
        matchLabels:
          field.cattle.io/projectId: p-v7sr7
      #serviceMonitorSelector:
      #  matchLabels:
      #    field.cattle.io/projectId: p-q8bp8
      #  matchNames:
      #  - demoapp3
      #  - demoapp4
          #matchLabels:
          #app: demoapp
      #serviceMonitorNamespaceSelector:
      #  matchNames:
      #  - demoapp3
      #  - demoapp4
      #  matchLabels:
      #    field.cattle.io/projectId: p-q8bp8
        #  app: demoapp
              #  matchNames:
              #   - demoapp3
              # - demoapp4
      shards: 1
      storageSpec: {}
      ## Using PersistentVolumeClaim
      ##
      #  volumeClaimTemplate:
      #    spec:
      #      storageClassName: gluster
      #      accessModes: ["ReadWriteOnce"]
      #      resources:
      #        requests:
      #          storage: 50Gi
      #    selector: {}
      ## Using tmpfs volume
      ##
      #  emptyDir:
      #    medium: Memory
      volumes: []
      volumeMounts: []
      walCompression: true
    serviceAccount:
      create: false
      name:  project-monitoring
    service:
      annotations: {}
      clusterIP: ""
      externalIPs: []
      externalTrafficPolicy: Cluster
      labels: {}
      port: 9090
      publishNotReadyAddresses: false
      targetPort: 9090
      type: ClusterIP
    servicePerReplica:
      enabled: false
    serviceMonitor:
      interval: "30s"
      selfMonitor: true
      scheme: ""
      tlsConfig: {}
      bearerTokenFile:
      metricRelabelings: []
      relabelings: []
  thanosRuler:
    enabled: false
